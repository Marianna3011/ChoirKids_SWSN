{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0fae62",
   "metadata": {},
   "source": [
    "## Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45159ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Fix for PyTorch 2.6 torch.load compatibility\n",
    "original_load = torch.load\n",
    "torch.load = lambda *args, **kwargs: original_load(*args, **{**kwargs, 'weights_only': False})\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9817681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (this may take a moment)...\n",
      "Dataset loaded:\n",
      "  Nodes: 2,449,029\n",
      "  Edges: 123,718,280\n",
      "  Features: 100\n",
      "  Classes: 47\n",
      "  Test nodes: 2,213,091\n",
      "Dataset loaded:\n",
      "  Nodes: 2,449,029\n",
      "  Edges: 123,718,280\n",
      "  Features: 100\n",
      "  Classes: 47\n",
      "  Test nodes: 2,213,091\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "\n",
    "print(\"Loading dataset (this may take a moment)...\")\n",
    "dataset = NodePropPredDataset(name='ogbn-products', root='../dataset')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
    "val_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
    "test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n",
    "\n",
    "graph, labels = dataset[0]\n",
    "node_features = torch.tensor(graph['node_feat'], dtype=torch.float)\n",
    "edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.long).squeeze()\n",
    "\n",
    "num_nodes = node_features.shape[0]\n",
    "num_classes = len(torch.unique(labels))\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Nodes: {num_nodes:,}\")\n",
    "print(f\"  Edges: {edge_index.shape[1]:,}\")\n",
    "print(f\"  Features: {node_features.shape[1]}\")\n",
    "print(f\"  Classes: {num_classes}\")\n",
    "print(f\"  Test nodes: {test_idx.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19bdbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency list created\n"
     ]
    }
   ],
   "source": [
    "# Build adjacency list for neighbor sampling\n",
    "adj_list = {i: [] for i in range(num_nodes)}\n",
    "for src, dst in edge_index.t().numpy():\n",
    "    adj_list[src].append(dst)\n",
    "    adj_list[dst].append(src)  # undirected\n",
    "\n",
    "print(\"Adjacency list created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86f921",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d7adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class SAGEConvLayer(torch.nn.Module):\n",
    "    \"\"\"GraphSAGE convolution layer - mean aggregation\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(2 * in_features, out_features)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x: [N, in_features]\n",
    "        # edge_index: [2, E]\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # Aggregate neighbors (mean aggregation)\n",
    "        agg = torch.zeros(x.size(0), x.size(1), device=x.device)\n",
    "        deg = torch.zeros(x.size(0), device=x.device)\n",
    "        \n",
    "        # Sum neighbors\n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg.index_add_(0, row, torch.ones(row.size(0), device=x.device))\n",
    "        \n",
    "        # Mean aggregation\n",
    "        deg = deg.clamp(min=1).unsqueeze(1)\n",
    "        agg = agg / deg\n",
    "        \n",
    "        # Concatenate self features with aggregated neighbor features\n",
    "        out = torch.cat([x, agg], dim=1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.convs.append(SAGEConvLayer(in_channels, hidden_channels))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConvLayer(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(SAGEConvLayer(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021aebd",
   "metadata": {},
   "source": [
    "## Sampling and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a840e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def sample_neighbors(nodes, adj_list, num_samples=[10, 5]):\n",
    "    \"\"\"\n",
    "    Sample neighbors for multiple hops\n",
    "    nodes: list of node indices\n",
    "    adj_list: dict mapping node -> list of neighbors\n",
    "    num_samples: list of number of neighbors to sample per hop\n",
    "    Returns: all sampled nodes and subgraph edges\n",
    "    \"\"\"\n",
    "    all_nodes = set(nodes)\n",
    "    current_layer = set(nodes)\n",
    "    edges = []\n",
    "    \n",
    "    for k in num_samples:\n",
    "        next_layer = set()\n",
    "        for node in current_layer:\n",
    "            neighbors = adj_list.get(node, [])\n",
    "            if len(neighbors) > 0:\n",
    "                # Sample k neighbors (or all if less than k)\n",
    "                sampled = np.random.choice(\n",
    "                    neighbors, \n",
    "                    size=min(k, len(neighbors)), \n",
    "                    replace=False\n",
    "                )\n",
    "                for neighbor in sampled:\n",
    "                    edges.append((node, neighbor))\n",
    "                    next_layer.add(neighbor)\n",
    "                    all_nodes.add(neighbor)\n",
    "        current_layer = next_layer\n",
    "    \n",
    "    return list(all_nodes), edges\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_sampling(model, node_feats, node_labels, mask, device, \n",
    "                           adj_list, batch_size=2048, num_samples=[10, 5], \n",
    "                           verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate model with specified neighbor sampling strategy\n",
    "    Returns: accuracy and inference time\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get indices of nodes to evaluate\n",
    "    eval_indices = mask.nonzero(as_tuple=True)[0].numpy()\n",
    "    num_eval = len(eval_indices)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process in batches with neighbor sampling\n",
    "    for i in range(0, num_eval, batch_size):\n",
    "        batch_target_nodes = eval_indices[i:i+batch_size]\n",
    "        \n",
    "        # Sample neighbors for this batch\n",
    "        sampled_nodes, sampled_edges = sample_neighbors(\n",
    "            batch_target_nodes.tolist(), adj_list, num_samples\n",
    "        )\n",
    "        node_map = {n: idx for idx, n in enumerate(sampled_nodes)}\n",
    "        \n",
    "        # Get subgraph features\n",
    "        batch_x = node_feats[sampled_nodes].to(device)\n",
    "        \n",
    "        # Create subgraph edge_index\n",
    "        subgraph_edges = []\n",
    "        for src, dst in sampled_edges:\n",
    "            if src in node_map and dst in node_map:\n",
    "                subgraph_edges.append([node_map[src], node_map[dst]])\n",
    "        \n",
    "        if len(subgraph_edges) > 0:\n",
    "            batch_edge_index = torch.tensor(subgraph_edges, dtype=torch.long).t().to(device)\n",
    "        else:\n",
    "            batch_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(batch_x, batch_edge_index)\n",
    "        \n",
    "        # Get predictions for target nodes only\n",
    "        target_indices = [node_map[n] for n in batch_target_nodes if n in node_map]\n",
    "        pred = out[target_indices].argmax(dim=1).cpu()\n",
    "        labels_batch = node_labels[batch_target_nodes[:len(target_indices)]]\n",
    "        \n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(labels_batch)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del out, batch_x, batch_edge_index\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    correct = (all_preds == all_labels).sum()\n",
    "    acc = correct.item() / len(all_labels)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluated {len(all_labels):,} nodes in {inference_time:.2f}s\")\n",
    "    \n",
    "    return acc, inference_time\n",
    "\n",
    "print(\"Sampling and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1465fe",
   "metadata": {},
   "source": [
    "## Load Saved GraphSAGE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83dddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GraphSAGE model loaded from checkpoint\n",
      "  Epoch: N/A\n",
      "  Train accuracy: N/A\n",
      "  Val accuracy: N/A\n",
      "  Test accuracy: N/A\n",
      "\n",
      "  Epoch: N/A\n",
      "  Train accuracy: N/A\n",
      "  Val accuracy: N/A\n",
      "  Test accuracy: N/A\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with same architecture\n",
    "in_channels = node_features.shape[1]\n",
    "hidden_channels = 256\n",
    "\n",
    "model = GraphSAGE(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=num_classes,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# Check for model file and load saved weights\n",
    "model_path = Path('..\\\\notebooks\\\\models\\\\graphsage_model.pth')\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(\"‚ùå ERROR: Model file not found!\")\n",
    "    print(f\"   Expected location: {model_path.absolute()}\")\n",
    "    print(\"\\nüìù To use this notebook, you need to:\")\n",
    "    print(\"   1. Open first_iteration.ipynb\")\n",
    "    print(\"   2. Train the GraphSAGE model (run all training cells)\")\n",
    "    print(\"   3. Make sure the model is saved to models/graphsage_model.pth\")\n",
    "    print(\"\\nAlternatively, check if the model was saved to a different location.\")\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì GraphSAGE model loaded from checkpoint\")\n",
    "print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Train accuracy: {checkpoint.get('train_acc', 'N/A')}\")\n",
    "print(f\"  Val accuracy: {checkpoint.get('val_acc', 'N/A')}\")\n",
    "print(f\"  Test accuracy: {checkpoint.get('test_acc', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0540b",
   "metadata": {},
   "source": [
    "## Create Test Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83f1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 2,213,091 nodes\n"
     ]
    }
   ],
   "source": [
    "# Create test mask\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask[test_idx.cpu()] = True\n",
    "\n",
    "print(f\"Test set: {test_mask.sum():,} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40916d5",
   "metadata": {},
   "source": [
    "## Test Different Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FAST MODE: Testing 3 sampling strategies (reduced from 8)\n",
      "üí° To test more strategies, uncomment lines in the cell above\n",
      "\n",
      "Testing sampling: [5, 3]\n"
     ]
    }
   ],
   "source": [
    "# Define different sampling strategies to test - REDUCED FOR SPEED\n",
    "sampling_strategies = [\n",
    "    [5, 3],      # Fast (fewer samples)\n",
    "    [10, 5],     # Baseline\n",
    "    [15, 10],    # More samples\n",
    "    # Commented out to save time - uncomment if needed:\n",
    "    # [20, 10],    # Even more first-hop\n",
    "    # [25, 15],    # High sampling\n",
    "    # [30, 15],    # Very high sampling\n",
    "    # [15, 5],     # More first-hop, fewer second-hop\n",
    "    # [10, 10],    # Balanced\n",
    "]\n",
    "\n",
    "print(f\"üöÄ FAST MODE: Testing {len(sampling_strategies)} sampling strategies (reduced from 8)\")\n",
    "print(f\"üí° To test more strategies, uncomment lines in the cell above\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for num_samples in sampling_strategies:\n",
    "    print(f\"Testing sampling: {num_samples}\")\n",
    "    \n",
    "    # Run evaluation 1 time only (reduced from 3 for speed)\n",
    "    acc, inf_time = evaluate_with_sampling(\n",
    "        model, \n",
    "        node_features.cpu(), \n",
    "        labels.cpu(), \n",
    "        test_mask, \n",
    "        device, \n",
    "        adj_list,\n",
    "        batch_size=4096,  # Increased from 2048 for speed\n",
    "        num_samples=num_samples,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Single run - no averaging needed\n",
    "    accs = [acc]\n",
    "    times = [inf_time]\n",
    "    \n",
    "    avg_acc = np.mean(accs)\n",
    "    std_acc = np.std(accs)\n",
    "    avg_time = np.mean(times)\n",
    "    \n",
    "    results.append({\n",
    "        'sampling_strategy': str(num_samples),\n",
    "        'hop1_samples': num_samples[0],\n",
    "        'hop2_samples': num_samples[1],\n",
    "        'test_accuracy': avg_acc,\n",
    "        'accuracy_std': std_acc,\n",
    "        'inference_time': avg_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚Üí Accuracy: {avg_acc:.4f} (¬±{std_acc:.4f}), Time: {avg_time:.2f}s\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All tests complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5553c78",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a73471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('test_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nResults ranked by test accuracy:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best strategy\n",
    "best_idx = results_df['test_accuracy'].idxmax()\n",
    "best_result = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST SAMPLING STRATEGY:\")\n",
    "print(f\"  Samples: {best_result['sampling_strategy']}\")\n",
    "print(f\"  Accuracy: {best_result['test_accuracy']:.4f} (¬±{best_result['accuracy_std']:.4f})\")\n",
    "print(f\"  Time: {best_result['inference_time']:.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34dd11",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Sampling Strategy\n",
    "x_labels = results_df['sampling_strategy'].values\n",
    "ax1.bar(range(len(results_df)), results_df['test_accuracy'], color='skyblue')\n",
    "ax1.errorbar(range(len(results_df)), results_df['test_accuracy'], \n",
    "             yerr=results_df['accuracy_std'], fmt='none', color='red', capsize=5)\n",
    "ax1.set_xticks(range(len(results_df)))\n",
    "ax1.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "ax1.set_xlabel('Sampling Strategy [hop1, hop2]')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Test Accuracy by Sampling Strategy')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy vs Inference Time Trade-off\n",
    "scatter = ax2.scatter(results_df['inference_time'], results_df['test_accuracy'], \n",
    "                      s=100, alpha=0.6, c=range(len(results_df)), cmap='viridis')\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax2.annotate(row['sampling_strategy'], \n",
    "                (row['inference_time'], row['test_accuracy']),\n",
    "                fontsize=8, alpha=0.7)\n",
    "ax2.set_xlabel('Inference Time (seconds)')\n",
    "ax2.set_ylabel('Test Accuracy')\n",
    "ax2.set_title('Accuracy vs Speed Trade-off')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35441c",
   "metadata": {},
   "source": [
    "## Test with Even More Aggressive Sampling (Optional - SKIP TO SAVE TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da08c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIPPED FOR SPEED - Uncomment to test aggressive sampling strategies\n",
    "\"\"\"\n",
    "aggressive_strategies = [\n",
    "    [50, 25],\n",
    "    [40, 30],\n",
    "    [100, 50],\n",
    "]\n",
    "\n",
    "print(\"Testing aggressive sampling strategies (may be slower)...\\n\")\n",
    "\n",
    "aggressive_results = []\n",
    "\n",
    "for num_samples in aggressive_strategies:\n",
    "    print(f\"Testing sampling: {num_samples}\")\n",
    "    \n",
    "    acc, inf_time = evaluate_with_sampling(\n",
    "        model, \n",
    "        node_features.cpu(), \n",
    "        labels.cpu(), \n",
    "        test_mask, \n",
    "        device, \n",
    "        adj_list,\n",
    "        batch_size=2048,\n",
    "        num_samples=num_samples,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    aggressive_results.append({\n",
    "        'sampling_strategy': str(num_samples),\n",
    "        'test_accuracy': acc,\n",
    "        'inference_time': inf_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚Üí Accuracy: {acc:.4f}, Time: {inf_time:.2f}s\\n\")\n",
    "\n",
    "aggressive_df = pd.DataFrame(aggressive_results)\n",
    "print(\"\\nAggressive sampling results:\")\n",
    "print(aggressive_df.to_string(index=False))\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚è© Aggressive sampling section SKIPPED to save time\")\n",
    "print(\"   Uncomment the code above if you want to test [50,25], [40,30], [100,50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abae41",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a58ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('../notebooks/sampling_strategy_results.csv', index=False)\n",
    "print(\"Results saved to sampling_strategy_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
