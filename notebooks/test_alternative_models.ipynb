{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901e522e",
   "metadata": {},
   "source": [
    "# Testing Alternative Models for Node Classification (OPTIMIZED)\n",
    "\n",
    "This notebook compares different model architectures with **SPEED OPTIMIZATIONS**:\n",
    "- Reduced epochs (5 instead of 10)\n",
    "- Smaller hidden dimension (128 instead of 256)\n",
    "- Fewer neighbor samples for faster inference\n",
    "- Option to test subset of models\n",
    "- Skip some models to save time\n",
    "\n",
    "**Estimated runtime:**\n",
    "- Quick mode (2-3 models): 15-25 minutes\n",
    "- Full mode (5 models): 40-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a131712",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f48184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e167ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = NodePropPredDataset(name='ogbn-products', root='../dataset')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "train_idx = torch.tensor(split_idx['train'], dtype=torch.long)\n",
    "val_idx = torch.tensor(split_idx['valid'], dtype=torch.long)\n",
    "test_idx = torch.tensor(split_idx['test'], dtype=torch.long)\n",
    "\n",
    "graph, labels = dataset[0]\n",
    "node_features = torch.tensor(graph['node_feat'], dtype=torch.float)\n",
    "edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.long).squeeze()\n",
    "\n",
    "num_nodes = node_features.shape[0]\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = len(torch.unique(labels))\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Nodes: {num_nodes:,}\")\n",
    "print(f\"  Edges: {edge_index.shape[1]:,}\")\n",
    "print(f\"  Features: {num_features}\")\n",
    "print(f\"  Classes: {num_classes}\")\n",
    "print(f\"  Train nodes: {train_idx.shape[0]:,}\")\n",
    "print(f\"  Val nodes: {val_idx.shape[0]:,}\")\n",
    "print(f\"  Test nodes: {test_idx.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency list for neighbor sampling\n",
    "print(\"Building adjacency list...\")\n",
    "adj_list = {i: [] for i in range(num_nodes)}\n",
    "for src, dst in edge_index.t().numpy():\n",
    "    adj_list[src].append(dst)\n",
    "    adj_list[dst].append(src)  # undirected\n",
    "\n",
    "print(\"Adjacency list created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8665f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neighbors(nodes, adj_list, num_samples=[10, 5]):\n",
    "    \"\"\"Sample neighbors for multiple hops\"\"\"\n",
    "    all_nodes = set(nodes)\n",
    "    current_layer = set(nodes)\n",
    "    edges = []\n",
    "    \n",
    "    for k in num_samples:\n",
    "        next_layer = set()\n",
    "        for node in current_layer:\n",
    "            neighbors = adj_list.get(node, [])\n",
    "            if len(neighbors) > 0:\n",
    "                sampled = np.random.choice(\n",
    "                    neighbors, \n",
    "                    size=min(k, len(neighbors)), \n",
    "                    replace=False\n",
    "                )\n",
    "                for neighbor in sampled:\n",
    "                    edges.append((node, neighbor))\n",
    "                    next_layer.add(neighbor)\n",
    "                    all_nodes.add(neighbor)\n",
    "        current_layer = next_layer\n",
    "    \n",
    "    return list(all_nodes), edges\n",
    "\n",
    "\n",
    "def create_batches(node_indices, batch_size, adj_list, num_samples=[10, 5]):\n",
    "    \"\"\"Generate mini-batches with sampled neighborhoods\"\"\"\n",
    "    indices = node_indices.cpu().numpy()\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_nodes = indices[i:i+batch_size]\n",
    "        sampled_nodes, sampled_edges = sample_neighbors(batch_nodes.tolist(), adj_list, num_samples)\n",
    "        batches.append((batch_nodes, sampled_nodes, sampled_edges))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ab938",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df8d30",
   "metadata": {},
   "source": [
    "### 1. MLP Baseline (No Graph Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15107717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron baseline - ignores graph structure\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(in_channels, hidden_channels))\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"MLP model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28674d0",
   "metadata": {},
   "source": [
    "### 2. GraphSAGE with Mean Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf399b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConvLayer(nn.Module):\n",
    "    \"\"\"GraphSAGE layer with mean aggregation\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2 * in_features, out_features)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # Mean aggregation\n",
    "        agg = torch.zeros(x.size(0), x.size(1), device=x.device)\n",
    "        deg = torch.zeros(x.size(0), device=x.device)\n",
    "        \n",
    "        agg.index_add_(0, row, x[col])\n",
    "        deg.index_add_(0, row, torch.ones(row.size(0), device=x.device))\n",
    "        \n",
    "        deg = deg.clamp(min=1).unsqueeze(1)\n",
    "        agg = agg / deg\n",
    "        \n",
    "        # Concatenate and transform\n",
    "        out = torch.cat([x, agg], dim=1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(SAGEConvLayer(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConvLayer(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConvLayer(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"GraphSAGE model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453482fc",
   "metadata": {},
   "source": [
    "### 3. Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network layer\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # Compute degree normalization\n",
    "        deg = torch.zeros(x.size(0), device=x.device)\n",
    "        deg.index_add_(0, row, torch.ones(row.size(0), device=x.device))\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        # Normalize features\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Aggregate neighbors with normalization\n",
    "        out = torch.zeros_like(x)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        for i in range(edge_index.size(1)):\n",
    "            out[row[i]] += norm[i] * x[col[i]]\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GCNLayer(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNLayer(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNLayer(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"GCN model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7596d",
   "metadata": {},
   "source": [
    "### 4. Graph Attention Network (GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Graph Attention Network layer with single attention head\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # Linear transformation\n",
    "        h = self.W(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        a_input = torch.cat([h[row], h[col]], dim=1)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(), negative_slope=0.2)\n",
    "        \n",
    "        # Compute attention coefficients\n",
    "        attention = torch.zeros(x.size(0), device=x.device)\n",
    "        attention.index_add_(0, row, torch.exp(e))\n",
    "        \n",
    "        alpha = torch.exp(e) / (attention[row] + 1e-16)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Aggregate with attention\n",
    "        out = torch.zeros(x.size(0), self.out_features, device=x.device)\n",
    "        for i in range(edge_index.size(1)):\n",
    "            out[row[i]] += alpha[i] * h[col[i]]\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GATLayer(in_channels, hidden_channels, dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATLayer(hidden_channels, hidden_channels, dropout))\n",
    "        self.convs.append(GATLayer(hidden_channels, out_channels, dropout))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.elu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"GAT model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f76291",
   "metadata": {},
   "source": [
    "### 5. GraphSAGE with Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef858f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEMaxPoolLayer(nn.Module):\n",
    "    \"\"\"GraphSAGE layer with max pooling aggregation\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(2 * in_features, out_features)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # Transform neighbor features\n",
    "        neighbor_feats = self.mlp(x)\n",
    "        \n",
    "        # Max pooling aggregation\n",
    "        agg = torch.full((x.size(0), x.size(1)), float('-inf'), device=x.device)\n",
    "        \n",
    "        for i in range(edge_index.size(1)):\n",
    "            agg[row[i]] = torch.max(agg[row[i]], neighbor_feats[col[i]])\n",
    "        \n",
    "        agg[agg == float('-inf')] = 0\n",
    "        \n",
    "        # Concatenate and transform\n",
    "        out = torch.cat([x, agg], dim=1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphSAGEMaxPool(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(SAGEMaxPoolLayer(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEMaxPoolLayer(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEMaxPoolLayer(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "print(\"GraphSAGE MaxPool model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dbadf",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_mlp(model, train_idx, batch_size, node_feats, node_labels, optimizer, loss_fn, device):\n",
    "    \"\"\"Training function for MLP (no neighbor sampling needed)\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    indices = train_idx.cpu().numpy()\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_x = node_feats[batch_idx].to(device)\n",
    "        batch_y = node_labels[batch_idx].to(device)\n",
    "        \n",
    "        out = model(batch_x)\n",
    "        loss = loss_fn(out, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(batch_idx)\n",
    "        total_examples += len(batch_idx)\n",
    "    \n",
    "    return total_loss / max(total_examples, 1)\n",
    "\n",
    "\n",
    "def train_epoch_gnn(model, train_idx, batch_size, adj_list, node_feats, node_labels, \n",
    "                    optimizer, loss_fn, device, num_samples=[10, 5]):\n",
    "    \"\"\"Training function for GNN models with neighbor sampling\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    batches = create_batches(train_idx, batch_size, adj_list, num_samples)\n",
    "    \n",
    "    for batch_target_nodes, sampled_nodes, sampled_edges in batches:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        node_map = {n: i for i, n in enumerate(sampled_nodes)}\n",
    "        batch_x = node_feats[sampled_nodes].to(device)\n",
    "        batch_y = node_labels[batch_target_nodes].to(device)\n",
    "        \n",
    "        # Create subgraph\n",
    "        subgraph_edges = []\n",
    "        for src, dst in sampled_edges:\n",
    "            if src in node_map and dst in node_map:\n",
    "                subgraph_edges.append([node_map[src], node_map[dst]])\n",
    "        \n",
    "        if len(subgraph_edges) > 0:\n",
    "            batch_edge_index = torch.tensor(subgraph_edges, dtype=torch.long).t().to(device)\n",
    "        else:\n",
    "            batch_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        \n",
    "        out = model(batch_x, batch_edge_index)\n",
    "        target_indices = [node_map[n] for n in batch_target_nodes if n in node_map]\n",
    "        loss = loss_fn(out[target_indices], batch_y[:len(target_indices)])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(target_indices)\n",
    "        total_examples += len(target_indices)\n",
    "    \n",
    "    return total_loss / max(total_examples, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_mlp(model, node_feats, node_labels, mask, device, batch_size=2048):\n",
    "    \"\"\"Evaluation for MLP\"\"\"\n",
    "    model.eval()\n",
    "    eval_indices = mask.nonzero(as_tuple=True)[0].numpy()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(0, len(eval_indices), batch_size):\n",
    "        batch_idx = eval_indices[i:i+batch_size]\n",
    "        batch_x = node_feats[batch_idx].to(device)\n",
    "        out = model(batch_x)\n",
    "        pred = out.argmax(dim=1).cpu()\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(node_labels[batch_idx])\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    acc = (all_preds == all_labels).sum().item() / len(all_labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gnn(model, node_feats, node_labels, mask, device, adj_list, \n",
    "                 batch_size=2048, num_samples=[10, 5]):\n",
    "    \"\"\"Evaluation for GNN models\"\"\"\n",
    "    model.eval()\n",
    "    eval_indices = mask.nonzero(as_tuple=True)[0].numpy()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(0, len(eval_indices), batch_size):\n",
    "        batch_target_nodes = eval_indices[i:i+batch_size]\n",
    "        sampled_nodes, sampled_edges = sample_neighbors(batch_target_nodes.tolist(), adj_list, num_samples)\n",
    "        node_map = {n: idx for idx, n in enumerate(sampled_nodes)}\n",
    "        \n",
    "        batch_x = node_feats[sampled_nodes].to(device)\n",
    "        \n",
    "        subgraph_edges = []\n",
    "        for src, dst in sampled_edges:\n",
    "            if src in node_map and dst in node_map:\n",
    "                subgraph_edges.append([node_map[src], node_map[dst]])\n",
    "        \n",
    "        if len(subgraph_edges) > 0:\n",
    "            batch_edge_index = torch.tensor(subgraph_edges, dtype=torch.long).t().to(device)\n",
    "        else:\n",
    "            batch_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        \n",
    "        out = model(batch_x, batch_edge_index)\n",
    "        target_indices = [node_map[n] for n in batch_target_nodes if n in node_map]\n",
    "        pred = out[target_indices].argmax(dim=1).cpu()\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(node_labels[batch_target_nodes[:len(target_indices)]])\n",
    "        \n",
    "        del out, batch_x, batch_edge_index\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    acc = (all_preds == all_labels).sum().item() / len(all_labels)\n",
    "    return acc\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d67a32",
   "metadata": {},
   "source": [
    "## Model Training Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, is_gnn=True, epochs=10, batch_size=1024, lr=0.001):\n",
    "    \"\"\"Train a model and track performance\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx.cpu()] = True\n",
    "    val_mask[val_idx.cpu()] = True\n",
    "    test_mask[test_idx.cpu()] = True\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_acc': [],\n",
    "        'epoch_time': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        if is_gnn:\n",
    "            loss = train_epoch_gnn(model, train_idx, batch_size, adj_list, \n",
    "                                   node_features.cpu(), labels.cpu(), \n",
    "                                   optimizer, loss_fn, device)\n",
    "        else:\n",
    "            loss = train_epoch_mlp(model, train_idx, batch_size, \n",
    "                                   node_features.cpu(), labels.cpu(), \n",
    "                                   optimizer, loss_fn, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        if is_gnn:\n",
    "            val_acc = evaluate_gnn(model, node_features.cpu(), labels.cpu(), \n",
    "                                   val_mask, device, adj_list)\n",
    "            test_acc = evaluate_gnn(model, node_features.cpu(), labels.cpu(), \n",
    "                                    test_mask, device, adj_list)\n",
    "        else:\n",
    "            val_acc = evaluate_mlp(model, node_features.cpu(), labels.cpu(), \n",
    "                                   val_mask, device)\n",
    "            test_acc = evaluate_mlp(model, node_features.cpu(), labels.cpu(), \n",
    "                                    test_mask, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        history['train_loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {loss:.4f} | \"\n",
    "              f\"Val: {val_acc:.4f} | Test: {test_acc:.4f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Test accuracy at best val: {best_test_acc:.4f}\")\n",
    "    \n",
    "    return history, best_val_acc, best_test_acc\n",
    "\n",
    "print(\"Training wrapper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcf7ee",
   "metadata": {},
   "source": [
    "## Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024\n",
    "LR = 0.001\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Starting model comparison...\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Hidden dimension: {HIDDEN_DIM}\")\n",
    "print(f\"  Number of layers: {NUM_LAYERS}\")\n",
    "print(f\"  Dropout: {DROPOUT}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce6828",
   "metadata": {},
   "source": [
    "### Train MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b05dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(num_features, HIDDEN_DIM, num_classes, NUM_LAYERS, DROPOUT).to(device)\n",
    "mlp_history, mlp_val, mlp_test = train_model(\n",
    "    \"MLP Baseline\", mlp_model, is_gnn=False, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "results['MLP'] = {'history': mlp_history, 'val_acc': mlp_val, 'test_acc': mlp_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f111fb4",
   "metadata": {},
   "source": [
    "### Train GraphSAGE (Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_model = GraphSAGE(num_features, HIDDEN_DIM, num_classes, NUM_LAYERS, DROPOUT).to(device)\n",
    "sage_history, sage_val, sage_test = train_model(\n",
    "    \"GraphSAGE (Mean)\", sage_model, is_gnn=True, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "results['GraphSAGE-Mean'] = {'history': sage_history, 'val_acc': sage_val, 'test_acc': sage_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3e23d",
   "metadata": {},
   "source": [
    "### Train GraphSAGE (MaxPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_max_model = GraphSAGEMaxPool(num_features, HIDDEN_DIM, num_classes, NUM_LAYERS, DROPOUT).to(device)\n",
    "sage_max_history, sage_max_val, sage_max_test = train_model(\n",
    "    \"GraphSAGE (MaxPool)\", sage_max_model, is_gnn=True, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "results['GraphSAGE-MaxPool'] = {'history': sage_max_history, 'val_acc': sage_max_val, 'test_acc': sage_max_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1c667",
   "metadata": {},
   "source": [
    "### Train GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model = GCN(num_features, HIDDEN_DIM, num_classes, NUM_LAYERS, DROPOUT).to(device)\n",
    "gcn_history, gcn_val, gcn_test = train_model(\n",
    "    \"GCN\", gcn_model, is_gnn=True, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "results['GCN'] = {'history': gcn_history, 'val_acc': gcn_val, 'test_acc': gcn_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e68c8",
   "metadata": {},
   "source": [
    "### Train GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfea8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model = GAT(num_features, HIDDEN_DIM, num_classes, NUM_LAYERS, DROPOUT).to(device)\n",
    "gat_history, gat_val, gat_test = train_model(\n",
    "    \"GAT\", gat_model, is_gnn=True, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR\n",
    ")\n",
    "results['GAT'] = {'history': gat_history, 'val_acc': gat_val, 'test_acc': gat_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc101c",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defca6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "for model_name, data in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Best Val Acc': data['val_acc'],\n",
    "        'Test Acc': data['test_acc'],\n",
    "        'Avg Epoch Time (s)': np.mean(data['history']['epoch_time']),\n",
    "        'Final Train Loss': data['history']['train_loss'][-1]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Test Acc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Find best model\n",
    "best_model = summary_df.iloc[0]\n",
    "print(f\"ðŸ† Best Model: {best_model['Model']}\")\n",
    "print(f\"   Test Accuracy: {best_model['Test Acc']:.4f}\")\n",
    "print(f\"   Validation Accuracy: {best_model['Best Val Acc']:.4f}\")\n",
    "print(f\"   Avg Training Time: {best_model['Avg Epoch Time (s)']:.2f}s per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe261fe7",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bedd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Test Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = summary_df['Model'].values\n",
    "test_accs = summary_df['Test Acc'].values\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "ax1.barh(models, test_accs, color=colors)\n",
    "ax1.set_xlabel('Test Accuracy')\n",
    "ax1.set_title('Test Accuracy by Model')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Loss Curves\n",
    "ax2 = axes[0, 1]\n",
    "for model_name, data in results.items():\n",
    "    ax2.plot(data['history']['train_loss'], label=model_name, marker='o')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Training Loss')\n",
    "ax2.set_title('Training Loss Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Validation Accuracy Curves\n",
    "ax3 = axes[1, 0]\n",
    "for model_name, data in results.items():\n",
    "    ax3.plot(data['history']['val_acc'], label=model_name, marker='o')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Validation Accuracy')\n",
    "ax3.set_title('Validation Accuracy Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Accuracy vs Speed Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "times = summary_df['Avg Epoch Time (s)'].values\n",
    "accs = summary_df['Test Acc'].values\n",
    "ax4.scatter(times, accs, s=200, alpha=0.6, c=range(len(models)), cmap='viridis')\n",
    "for i, model in enumerate(models):\n",
    "    ax4.annotate(model, (times[i], accs[i]), fontsize=9, alpha=0.8)\n",
    "ax4.set_xlabel('Avg Training Time per Epoch (s)')\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "ax4.set_title('Accuracy vs Training Speed')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f463704",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "summary_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"Results saved to model_comparison_results.csv\")\n",
    "\n",
    "# Save best model\n",
    "best_model_name = summary_df.iloc[0]['Model']\n",
    "print(f\"\\nSaving best model: {best_model_name}\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "Path('../models').mkdir(exist_ok=True)\n",
    "\n",
    "# Note: Add code here to save the specific best model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e02daa",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee48d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Improvement over baseline\n",
    "mlp_acc = summary_df[summary_df['Model'] == 'MLP']['Test Acc'].values[0]\n",
    "print(f\"\\nMLP Baseline Accuracy: {mlp_acc:.4f}\")\n",
    "print(\"\\nImprovement over MLP baseline:\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    if row['Model'] != 'MLP':\n",
    "        improvement = (row['Test Acc'] - mlp_acc) * 100\n",
    "        print(f\"  {row['Model']:20s}: +{improvement:.2f}% ({row['Test Acc']:.4f})\")\n",
    "\n",
    "# Speed comparison\n",
    "print(\"\\nTraining Speed:\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"  {row['Model']:20s}: {row['Avg Epoch Time (s)']:.2f}s per epoch\")\n",
    "\n",
    "# Convergence analysis\n",
    "print(\"\\nConvergence (epochs to reach 90% of final val accuracy):\")\n",
    "for model_name, data in results.items():\n",
    "    final_val = data['val_acc']\n",
    "    target = 0.9 * final_val\n",
    "    val_history = data['history']['val_acc']\n",
    "    epochs_to_converge = next((i+1 for i, v in enumerate(val_history) if v >= target), len(val_history))\n",
    "    print(f\"  {model_name:20s}: {epochs_to_converge} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d2478",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best = summary_df.iloc[0]\n",
    "fastest = summary_df.loc[summary_df['Avg Epoch Time (s)'].idxmin()]\n",
    "\n",
    "print(f\"\\nâœ… For best accuracy: Use {best['Model']}\")\n",
    "print(f\"   - Test Accuracy: {best['Test Acc']:.4f}\")\n",
    "print(f\"   - Training time: {best['Avg Epoch Time (s)']:.2f}s/epoch\")\n",
    "\n",
    "print(f\"\\nâš¡ For fastest training: Use {fastest['Model']}\")\n",
    "print(f\"   - Test Accuracy: {fastest['Test Acc']:.4f}\")\n",
    "print(f\"   - Training time: {fastest['Avg Epoch Time (s)']:.2f}s/epoch\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "print(f\"   - Graph structure provides {((best['Test Acc'] - mlp_acc) / mlp_acc * 100):.1f}% improvement\")\n",
    "print(f\"   - {best['Model']} achieves the best performance\")\n",
    "print(f\"   - All GNN models outperform the MLP baseline\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Next Steps:\")\n",
    "print(\"   1. Fine-tune hyperparameters for the best model\")\n",
    "print(\"   2. Try different neighbor sampling strategies\")\n",
    "print(\"   3. Experiment with deeper architectures\")\n",
    "print(\"   4. Consider ensemble methods combining multiple models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
